My project focuses on cross-modal contrastive learning across three modalities: H&E images, Visium spot-level gene expression data, and CODEX subcellular or pixel-level protein expression data. The primary goal is to train a foundation model that learns representations capturing tissue morphology, highly variable gene expression, spatial microenvironment context, and marker-level protein expression, for downstream functional modules.

Visium data and H&E images are acquired from the same tissue section. CODEX data are acquired from an adjacent section of the same tissue block. Therefore, the CODEX section may exhibit subtle morphological differences and may require additional alignment methods. I thus first focus on modeling H&E images and Visium data and incorporate the CODEX modality after the method is well established.

There are two core questions in encoding H&E images and Visium data. First, unlike previous foundation model training on single-cell transcriptomics, Visium encoding is spot-level rather than gene-level. This means that, for each spot, all highly variable genes must be encoded in a biologically meaningful spot-level representation (spot × feature), rather than each gene in a gene-level representation (gene × feature).

Second, if each spot is encoded only as a spot-level representation (spot × feature), each representation captures only the local gene expression state. It does not capture the spatial microenvironment or spatial context of that location. Therefore, spatial context, including local neighborhoods and global structure, must be injected into the embedding space representations.

For the first core question, I use a lightweight MLP to independently encode each spot’s HVG vector into a spot embedding. After Visium normalization and HVG selection (assume 500 genes), the input for spot $i$ is $x_i = [g_1, g_2, g_3, \ldots, g_{500}]$. The input for a batch of spots is $X_b = [x_1, x_2, x_3, \ldots, x_b]$. If the semantic space dimension is set to 256, the output is a set of 256-dimensional vectors $\{z_i\}$, where each vector represents the transcriptional state of the corresponding spot.

For the second core question, I encode the surrounding microenvironment of spot $i$ into $z_i$ to obtain a microenvironment-aware representation. Thus, spatial context is modeled on the representation ($z_i$) rather than on the raw expression. I use kNN to define the local and global neighborhoods of spot $i$. The local neighborhood consists of the 12 nearest spots ($k = 12$). The global neighborhood consists of the 96 nearest spots ($k = 96$).

I define three types of information: local $[z_i^{Local}]$, global $[z_i^{Global}]$, and contrast $[\Delta_i = z_i^{Local} - z_i^{Global}]$. They represent the local neighborhood, the global neighborhood, and boundary or transition regions, respectively. I then fuse these with the original representation $z_i$ using an MLP: $z_i^{c} = \mathrm{MLP}(\mathrm{concat}[z_i, z_i^{Local}, z_i^{Global}, \Delta_i])$. Each resulting representation encodes the spot’s transcriptional state, local microenvironment context, regional background, and boundary or transition signals.

(Jan 27 update) The current version uses mean pooling to compute $z_i^{Local}$ and $z_i^{Global}$. The neighborhood size of $z_i^{Local}$ and $z_i^{Global}$ is determined by the manually set kNN parameter $k$. For example, if the local neighborhood uses $k = 12$, then the local neighborhood size is $L = 13$, including the spot itself and its 12 neighboring spots.

(Jan 27 update) The semantic meaning of $\Delta_i$ captures the deviation of the local microenvironment of spot $i$ from its regional tissue background. Regions such as tumor–stroma boundaries, immune infiltration fronts, or necrotic margins are typically not defined by absolute expression levels, but by abnormality relative to surrounding tissue. $\Delta_i$ explicitly highlights and distinguishes these spatial transition regions.


